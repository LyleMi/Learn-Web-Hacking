站点信息收集
================================

- 扫描敏感文件
    - robots.txt
    - crossdomain.xml
    - sitemap.xml
    - xx.tar.gz
    - xx.bak
    - 等

- 确定网站采用的语言、框架、中间件服务器、第三方库
    - 查看源代码
    - 找后缀，比如php/asp/jsp
    - 看header里面有没有信息
    - 根据Cookie判断
    - 根据报错信息判断
    - 根据默认页面判断
    - 根据css 图片等资源的hash值判断

- 探测有没有WAF，如果有，什么类型的
    - 有WAF，找绕过方式
    - 没有，进入下一步

- 扫描敏感目录，看是否存在信息泄漏
    - 扫描之前先自己尝试几个的url，人为看看反应

- 使用爬虫爬取网站信息

- 拿到一定信息后，通过拿到的目录名称，文件名称及文件扩展名了解网站开发人员的命名思路，确定其命名规则，推测出更多的目录及文件名

